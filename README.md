# KK's Paperlist
List papers for machine learning, reinforcement learning, NLP or something interesting.
Recommendations welcome!

## Table of Contents
- [Natural Language Inference](https://github.com/SparkJiao/KK-s-Paperlist#natural-language-inference)
- [Machine Reading Comprehension](https://github.com/SparkJiao/KK-s-Paperlist#machine-reading-comprehension)
- [Machine Translation](https://github.com/SparkJiao/KK-s-Paperlist#machine-translation)
- [Interpretability of Machine Learning](https://github.com/SparkJiao/KK-s-Paperlist#interpretability-of-machine-learning)
- [Attention Mechanism](https://github.com/SparkJiao/KK-s-Paperlist#attention-mechanism)
- [Reinforcement Learning](https://github.com/SparkJiao/KK-s-Paperlist#reinforcement-learning)
- [Dialog System](https://github.com/SparkJiao/KK-s-Paperlist#dialog-system)
- [Graph Neural Network](https://github.com/SparkJiao/KK-s-Paperlist#graph-neural-network)
- [Theory](https://github.com/SparkJiao/KK-s-Paperlist#theory)
- [Waiting List](https://github.com/SparkJiao/KK-s-Paperlist#waiting-list)
- [Some Interesting or Useful Blogs](https://github.com/SparkJiao/KK-s-Paperlist#some-interesting-or-useful-blogs)

## Natural Language Inference
[DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching](https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangKun.5147.pdf)

## Machine Reading Comprehension
[DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension](https://arxiv.org/abs/1902.00164)

[Review Conversational Reading Comprehension](https://arxiv.org/abs/1902.00821)

[FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://arxiv.org/abs/1810.06683)
- Using RNN to grasp historical information in conversational question answering.

[SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering](https://arxiv.org/abs/1812.03593)
- Generate bert embedding for reading comprehensing and question answering.

[FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension](https://arxiv.org/abs/1711.07341)
- Extend existing attention approaches from three perspectives.

[Densely Connected Attention Propagation for Reading Comprehension](https://arxiv.org/abs/1811.04210)
- Propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension.

[S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension](https://arxiv.org/abs/1706.04815)

[QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541)

[Reinforced Mnemonic Reader for Machine Reading Comprehension](https://arxiv.org/pdf/1705.02798.pdf)

[Multihop Attention Networks for Question Answer Matching](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/Multihop%20Attention%20Networks%20for%20Question%20Answer%20Matching.pdf)

[DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://arxiv.org/abs/1903.00161)

[Multi-Matching Network for Multiple Choice Reading Comprehension](http://xplan-lab.org/Paper_PDF/AAAI-19.pdf)


## Dialog System
[An Efficient Approach to Encoding Context for Spoken Language Understanding](https://arxiv.org/pdf/1807.00267.pdf)


## Attention Mechanism
[An Introductory Survey on Attention Mechanisms in NLP Problems](https://arxiv.org/abs/1811.05544)

[Modeling Localness for Self-Attention Networks](https://arxiv.org/abs/1810.10182)

[Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling](https://arxiv.org/abs/1809.01557)

[How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues](https://www.csie.ntu.edu.tw/~yvchen/doc/NAACL18_TimeDecay.pdf)

[Context-Aware Self-Attention Networks](https://arxiv.org/pdf/1902.05766.pdf)
- Use summarized vectors of context or hidden states of context to add extra contextual information into the process of calculating the similarity of between each word in KEY and VALUE.

[Document Modeling with External Attention for Sentence Extraction](http://aclweb.org/anthology/P18-1188)

## Machine Translation
[DTMT: A Novel Deep Transition Architecture for Neural Machine Translation](https://arxiv.org/pdf/1812.07807.pdf)
- Tap the potential strength of deep transition between consecutive hidden states and propose a novel deep transition RNN-based architecture for NMT
- Propose a simple yet more effective linear transformation enhanced GRU for our deep transition RNMT, which provides a linear transformation path for deep transition of consecutive hidden states.

## Interpretability of Machine Learning
[Interpretable machine learning: deÔ¨Ånitions, methods, and applications](https://arxiv.org/pdf/1901.04592.pdf)

## Reinforcement Learning
[Learning Structured Representation for Text Classification via Reinforcement Learning](http://coai.cs.tsinghua.edu.cn/hml/media/files/AAAI2018_ClassifyAndStructure.pdf)

## Graph Neural Network
[Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153)

## Theory
[On the Impact of the Activation Function on Deep Neural Networks Training](https://arxiv.org/pdf/1902.06853.pdf)

## Waiting List
[Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering](https://arxiv.org/pdf/1812.02354.pdf)

[BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671)

## Some Interesting or Useful Blogs
[Comparing Pre-trained Language Models with Semantic Parsing](https://jbkjr.com/posts/2019/01/unsupervised_pretraining_comparison/)

[Jack Koch's Blog](https://jbkjr.com/year-archive/)

[Attention mechanism paper from another repo](https://github.com/yuquanle/Attention-Mechanisms-paper/blob/master/Attention-mechanisms-paper.md)

[NAACL 2019 Accepted Papers](https://naacl2019.org/program/accepted/)
