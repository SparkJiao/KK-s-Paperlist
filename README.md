# KK's Paperlist
List papers for machine learning, reinforcement learning, NLP or something interesting.
Recommendations welcome!

## Table of Contents
- [Natural Language Inference](https://github.com/SparkJiao/KK-s-Paperlist#natural-language-inference)
- [Machine reading comprehension](https://github.com/SparkJiao/KK-s-Paperlist#machine-reading-comprehension)
- [Machine translation](https://github.com/SparkJiao/KK-s-Paperlist#machine-translation)
- [Interpretability of Machine Learning](https://github.com/SparkJiao/KK-s-Paperlist#interpretability-of-machine-learning)
- [Attention machanism](https://github.com/SparkJiao/KK-s-Paperlist#attention-mechanism)
- [Reinforcement learning](https://github.com/SparkJiao/KK-s-Paperlist#reinforcement-learning)
- [Dialog System](https://github.com/SparkJiao/KK-s-Paperlist#dialog-system)
- [Waiting List](https://github.com/SparkJiao/KK-s-Paperlist#waiting-list)
- [Some interesting or useful blogs](https://github.com/SparkJiao/KK-s-Paperlist#some-interesting-or-useful-blogs)

## Natural Language Inference
[DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching](https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangKun.5147.pdf)

## Machine Reading Comprehension
[DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension](https://arxiv.org/abs/1902.00164)

[Review Conversational Reading Comprehension](https://arxiv.org/abs/1902.00821)

[FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://arxiv.org/abs/1810.06683)
- Using RNN to grasp historical information in conversational question answering.

[SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering](https://arxiv.org/abs/1812.03593)
- Generate bert embedding for reading comprehensing and question answering.

[FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension](https://arxiv.org/abs/1711.07341)
- Extend existing attention approaches from three perspectives.

[Densely Connected Attention Propagation for Reading Comprehension](https://arxiv.org/abs/1811.04210)
- Propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension.

[S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension](https://arxiv.org/abs/1706.04815)

[QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541)

## Dialog System
[An Efficient Approach to Encoding Context for Spoken Language Understanding](https://arxiv.org/pdf/1807.00267.pdf)


## Attention Mechanism
[An Introductory Survey on Attention Mechanisms in NLP Problems](https://arxiv.org/abs/1811.05544)

[Modeling Localness for Self-Attention Networks](https://arxiv.org/abs/1810.10182)

[Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling](https://arxiv.org/abs/1809.01557)

[How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues](https://www.csie.ntu.edu.tw/~yvchen/doc/NAACL18_TimeDecay.pdf)

[Context-Aware Self-Attention Networks](https://arxiv.org/pdf/1902.05766.pdf)

## Machine Translation
[DTMT: A Novel Deep Transition Architecture for Neural Machine Translation](https://arxiv.org/pdf/1812.07807.pdf)
- Tap the potential strength of deep transition between consecutive hidden states and propose a novel deep transition RNN-based architecture for NMT
- Propose a simple yet more effective linear transformation enhanced GRU for our deep transition RNMT, which provides a linear transformation path for deep transition of consecutive hidden states.

## Interpretability of Machine Learning
[Interpretable machine learning: deÔ¨Ånitions, methods, and applications](https://arxiv.org/pdf/1901.04592.pdf)

## Reinforcement Learning
[Learning Structured Representation for Text Classification via Reinforcement Learning](http://coai.cs.tsinghua.edu.cn/hml/media/files/AAAI2018_ClassifyAndStructure.pdf)

## Waiting List
[Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering](https://arxiv.org/pdf/1812.02354.pdf)

[BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671)

## Some interesting or useful blogs
[Comparing Pre-trained Language Models with Semantic Parsing](https://jbkjr.com/posts/2019/01/unsupervised_pretraining_comparison/)

[Jack Koch's Blog](https://jbkjr.com/year-archive/)

