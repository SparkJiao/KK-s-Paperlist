# KK's Paperlist
A list of papers for machine learning, deep learning, reinforcement learning, NLP and something interesting.  
Recommendations or contributions are all welcome!

## Table of Contents
- [Bentchmark or Datasets](https://github.com/SparkJiao/KK-s-Paperlist#benchmark-or-datasets)
- [Language Models](https://github.com/SparkJiao/KK-s-Paperlist#language-models)
- [Visual Question Answering](https://github.com/SparkJiao/KK-s-Paperlist#visual-question-answering)
- [Natural Language Inference](https://github.com/SparkJiao/KK-s-Paperlist#natural-language-inference)
- [Machine Reading Comprehension](https://github.com/SparkJiao/KK-s-Paperlist#machine-reading-comprehension)
- [Open Domain Question Answering(Information Retrieval)](https://github.com/SparkJiao/KK-s-Paperlist#open-domain-question-answeringinformation-retrieval)
- [Claim Verification](https://github.com/SparkJiao/KK-s-Paperlist#claim-verification)
- [Document Summary](https://github.com/SparkJiao/KK-s-Paperlist#document-summary)
- [Commonsense Reasoning](https://github.com/SparkJiao/KK-s-Paperlist#commonsense-reasoning)
- [Machine Translation](https://github.com/SparkJiao/KK-s-Paperlist#machine-translation)
- [Interpretability of Machine Learning](https://github.com/SparkJiao/KK-s-Paperlist#interpretability-of-machine-learning)
- [Multi-Task Learning](https://github.com/SparkJiao/KK-s-Paperlist#multi-task-learning)
- [Transfer Learning](https://github.com/SparkJiao/KK-s-Paperlist#transfer-learning)
- [Attention Mechanism](https://github.com/SparkJiao/KK-s-Paperlist#attention-mechanism)
- [Reinforcement Learning](https://github.com/SparkJiao/KK-s-Paperlist#reinforcement-learning)
- [Dialog System](https://github.com/SparkJiao/KK-s-Paperlist#dialog-system)
- [Graph Neural Network](https://github.com/SparkJiao/KK-s-Paperlist#graph-neural-network)
- [Self Training](https://github.com/SparkJiao/KK-s-Paperlist#self-training)
- [Semi-Supervised Learning](https://github.com/SparkJiao/KK-s-Paperlist#semi-supervised-learning)
- [GAN](https://github.com/SparkJiao/KK-s-Paperlist#gan)
- [Theory](https://github.com/SparkJiao/KK-s-Paperlist#theory)
- [Waiting List](https://github.com/SparkJiao/KK-s-Paperlist#waiting-list)
- [Some Interesting or Useful Blogs](https://github.com/SparkJiao/KK-s-Paperlist#some-interesting-or-useful-blogs)

## Benchmark or Datasets
[CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog](https://arxiv.org/abs/1903.03166)

[DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension](https://arxiv.org/abs/1902.00164)

[DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://arxiv.org/abs/1903.00161)

[A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR)](https://arxiv.org/pdf/1903.11783.pdf)

[SocialIQA: Commonsense Reasoning about Social Interactions](https://arxiv.org/abs/1904.09728v1#)

[HEAD-QA: A Healthcare Dataset for Complex Reasoning](https://arxiv.org/abs/1906.04701v1)

## Language Models

[Conditional BERT Contextual Augmentation](https://arxiv.org/abs/1812.06705)

[Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/pdf/1903.12136v1.pdf)

[Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/abs/1904.09482v1)

[ERNIE: Enhanced Language Representation with Informative Entities](https://github.com/thunlp/ERNIE)

[Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)

[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

[Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/pdf/1903.08855.pdf)

## Visual Question Answering
[CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog](https://arxiv.org/abs/1903.03166)

[Visual Dialog](https://arxiv.org/abs/1611.08669)
- Late Fusion
- Hierarchical Recurrent Encoder
- Memory Network
- [Github is here.](https://github.com/batra-mlp-lab/visdial)  

[Learning to Reason: End-to-End Module Networks for Visual Question Answering](https://github.com/ronghanghu/n2nmn)  

[Visual Coreference Resolution in Visual Dialog using Neural Module Networks](https://github.com/facebookresearch/corefnmn)

## Natural Language Inference
[DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching](https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangKun.5147.pdf)

[Neural Natural Language Inference Models Enhanced with External Knowledge](https://aclweb.org/anthology/P18-1224)

[Knowledge Base Relation Detection via Multi-View Matching](https://arxiv.org/pdf/1803.00612.pdf)

## Machine Reading Comprehension
[DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension](https://arxiv.org/abs/1902.00164)

[Review Conversational Reading Comprehension](https://arxiv.org/abs/1902.00821)

[FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://arxiv.org/abs/1810.06683)
- Using RNN to grasp historical information in conversational question answering.

[SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering](https://arxiv.org/abs/1812.03593)
- Generate bert embedding for reading comprehensing and question answering.

[FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension](https://arxiv.org/abs/1711.07341)
- Extend existing attention approaches from three perspectives.

[Densely Connected Attention Propagation for Reading Comprehension](https://arxiv.org/abs/1811.04210)
- Propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension.

[S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension](https://arxiv.org/abs/1706.04815)

[QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541)

[U-Net: Machine Reading Comprehension with Unanswerable Questions](https://arxiv.org/abs/1810.06638)

[Reinforced Mnemonic Reader for Machine Reading Comprehension](https://arxiv.org/pdf/1705.02798.pdf)

[Read + Verify: Machine Reading Comprehension with Unanswerable Questions](https://arxiv.org/pdf/1808.05759.pdf)

[Multihop Attention Networks for Question Answer Matching](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/Multihop%20Attention%20Networks%20for%20Question%20Answer%20Matching.pdf)

[DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://arxiv.org/abs/1903.00161)

[Multi-Matching Network for Multiple Choice Reading Comprehension](http://xplan-lab.org/Paper_PDF/AAAI-19.pdf)

[Option Comparison Network for Multiple-choice Reading Comprehension](https://arxiv.org/abs/1903.03033)

[Dual Co-Matching Network for Multi-choice Reading Comprehension](https://arxiv.org/abs/1901.09381)

[Hierarchical Attention Flow for Multiple-Choice Reading Comprehension](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16331/16177)

[Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases](https://arxiv.org/pdf/1903.02188.pdf)

[SQuAD-MARS](https://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/80013584)

[Coarse-to-Fine Question Answering for Long Documents](https://aclweb.org/anthology/P17-1020)

[Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions](https://arxiv.org/pdf/1811.08610.pdf)

[A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension](https://arxiv.org/abs/1905.12848)

[Cognitive Graph for Multi-Hop Reading Comprehension at Scale](https://arxiv.org/pdf/1905.05460.pdf)

[GRAPHFLOW: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension](https://graphreason.github.io/papers/13.pdf)

[Neural Machine Reading Comprehension: Methods and Trends](https://arxiv.org/abs/1907.01118)

## Open Domain Question Answering(Information Retrieval)

[Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/pdf/1906.00300v3.pdf)

## Claim Verification

[Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks](https://www.researchgate.net/profile/Wei_Gao56/publication/333601369_Sentence-Level_Evidence_Embedding_for_Claim_Verification_with_Hierarchical_Attention_Networks/links/5cf6f755a6fdcc84750637eb/Sentence-Level-Evidence-Embedding-for-Claim-Verification-with-Hierarchical-Attention-Networks.pdf)

## Document Summarization

[Hierarchical Transformers for Multi-Document Summarization](https://arxiv.org/pdf/1905.13164v1.pdf)

[Self-Supervised Learning for Contextualized Extractive Summarization](https://arxiv.org/pdf/1906.04466.pdf)

[Fine-tune BERT for Extractive Summarization](https://arxiv.org/pdf/1903.10318.pdf)

## Commonsense Reasoning

[Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)

[Attention Is (not) All You Need for Commonsense Reasoning](https://arxiv.org/abs/1905.13497v1)

## Dialog System
[An Efficient Approach to Encoding Context for Spoken Language Understanding](https://arxiv.org/pdf/1807.00267.pdf)

[The Second Conversational Intelligence Challenge (ConvAI2)](https://arxiv.org/pdf/1902.00098.pdf)

[A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues](https://arxiv.org/pdf/1605.06069v3.pdf)

[Memory-augmented Dialogue Management for Task-oriented Dialogue Systems](https://arxiv.org/abs/1805.00150)

[Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling](https://arxiv.org/pdf/1906.06893.pdf)


## Attention Mechanism
[An Empirical Study of Spatial Attention Mechanisms in Deep Networks](https://arxiv.org/abs/1904.05873)

[An Introductory Survey on Attention Mechanisms in NLP Problems](https://arxiv.org/abs/1811.05544)

[Modeling Localness for Self-Attention Networks](https://arxiv.org/abs/1810.10182)

[Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling](https://arxiv.org/abs/1809.01557)

[How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues](https://www.csie.ntu.edu.tw/~yvchen/doc/NAACL18_TimeDecay.pdf)

[Context-Aware Self-Attention Networks](https://arxiv.org/pdf/1902.05766.pdf)
- Use summarized vectors of context or hidden states of context to add extra contextual information into the process of calculating the similarity of between each word in KEY and VALUE.

[Document Modeling with External Attention for Sentence Extraction](http://aclweb.org/anthology/P18-1188)

[Convolutional Self-Attention Networks](https://arxiv.org/pdf/1904.03107.pdf)

[Are Sixteen Heads Really Better than One?](https://arxiv.org/abs/1905.10650v2)


## Machine Translation
[DTMT: A Novel Deep Transition Architecture for Neural Machine Translation](https://arxiv.org/pdf/1812.07807.pdf)
- Tap the potential strength of deep transition between consecutive hidden states and propose a novel deep transition RNN-based architecture for NMT
- Propose a simple yet more effective linear transformation enhanced GRU for our deep transition RNMT, which provides a linear transformation path for deep transition of consecutive hidden states.

## Interpretability of Machine Learning
[Interpretable machine learning: deﬁnitions, methods, and applications](https://arxiv.org/pdf/1901.04592.pdf)

[Frustratingly Poor Performance of Reading Comprehension Models on Non-adversarial Examples](https://arxiv.org/pdf/1904.02665v1.pdf)

## Multi-Task Learning
[Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/pdf/1901.11504.pdf)
- Better performance than BERT on natural language understanding tasks.

[Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering](https://arxiv.org/pdf/1812.02354.pdf)

## Transfer Learning

[An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://arxiv.org/abs/1902.10547v3)

[MULTIQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension](https://arxiv.org/pdf/1905.13453v1.pdf)

[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751v2.pdf)

[HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf)

[Pretraining Methods for Dialog Context Representation Learning](https://arxiv.org/pdf/1906.00414.pdf)

## Reinforcement Learning
[Learning Structured Representation for Text Classification via Reinforcement Learning](http://coai.cs.tsinghua.edu.cn/hml/media/files/AAAI2018_ClassifyAndStructure.pdf)

[Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data](https://arxiv.org/abs/1903.06164)

## Graph Neural Network
[Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153)

[Embedding Logical Queries on Knowledge Graphs](https://papers.nips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs.pdf)

## Self Training

[Improve Diverse Text Generation by Self Labeling Conditional Variational Auto Encoder](https://arxiv.org/abs/1903.10842v1)

[Neural Self Talk: Image Understanding via Continuous Questioning and Answering](https://arxiv.org/abs/1512.03460v1)

[Dual Supervised Learning for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.06196v1)

[Structured Minimally Supervised Learning for Neural Relation Extraction](https://arxiv.org/abs/1904.00118v4)

[A Variational Approach to Weakly Supervised Document-Level Multi-Aspect Sentiment Classification](https://arxiv.org/abs/1904.05055v1)

[Effectiveness of Self Normalizing Neural Networks for Text Classification](https://arxiv.org/abs/1905.01338v1)

[Self-Supervised Learning for Contextualized Extractive Summarization](https://arxiv.org/abs/1906.04466v1)

[Self-Supervised Dialogue Learning](https://arxiv.org/pdf/1907.00448v1.pdf)

## Semi-Supervised Learning

[Tri-Training: Exploiting Unlabeled Data Using Three Classifiers](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1512038)

[Tri-net for Semi-Supervised Deep Learning](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/ijcai18trinet.pdf)

[Semi-Supervised Learning with Ladder Networks](https://arxiv.org/pdf/1507.02672.pdf)

[Zoho at SemEval-2019 Task 9: Semi-supervised Domain Adaptation using Tri-training for Suggestion Mining](https://arxiv.org/pdf/1902.10623v2.pdf)

[Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/pdf/1904.12848v2.pdf)

## GAN

[Adversarial Training Methods for Semi-Supervised Text Classification](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/ADVERSARIAL%20TRAINING%20METHODS%20FOR%20SEMI-SUPERVISED%20TEXT%20CLASSIFICATION.pdf)

[Adversarial Examples for Natural Language Classification Problems](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/Adversarial%20Examples%20for%20Natural%20Language%20Classification%20Problems.pdf)

## Theory
[On the Impact of the Activation Function on Deep Neural Networks Training](https://arxiv.org/pdf/1902.06853.pdf)

## Waiting List
[Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data](https://arxiv.org/abs/1903.00138v3)

[Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension](https://arxiv.org/pdf/1906.05210v1.pdf)

[Towards Explainable NLP: A Generative Explanation Framework for Text Classification](https://arxiv.org/abs/1811.00196v2)

[Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/pdf/1906.04413v1.pdf)

[Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network](https://arxiv.org/pdf/1906.04684v1.pdf)

[Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension](https://arxiv.org/pdf/1906.04618v1.pdf)

[Compositional Questions Do Not Necessitate Multi-hop Reasoning](https://arxiv.org/pdf/1906.02900.pdf)

[Multi-hop Reading Comprehension through Question Decomposition and Rescoring](https://arxiv.org/pdf/1906.02916.pdf)

[Selfie: Self-supervised Pretraining for Image Embedding](https://arxiv.org/pdf/1906.02940.pdf)

[BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671)

[Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing](https://www.semanticscholar.org/paper/Attention%2C-please!-A-Critical-Review-of-Neural-in-Galassi-Lippi/9a7def005efb5b4984886c8a07ec4d80152602ab)

[Fast LSTMs in PyTorch](https://lernapparat.de/fast-lstm-pytorch/)

[Random Language Model](https://arxiv.org/abs/1809.01201v2)

[Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study](https://arxiv.org/pdf/1904.05584.pdf)

[Understanding the Behaviors of BERT in Ranking](https://arxiv.org/abs/1904.07531)

[Relational Graph Attention Networks](https://arxiv.org/abs/1904.05811)

[Dry, Focus, and Transcribe: End-to-End Integration of Dereverberation, Beamforming, and ASR](https://arxiv.org/abs/1904.09049v2)

[Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System](https://arxiv.org/abs/1904.09636v1)

[Dynamic Past and Future for Neural Machine Translation](https://arxiv.org/abs/1904.09646v1)

[Probing Prior Knowledge Needed in Challenging Chinese Machine Reading Comprehension](https://arxiv.org/abs/1904.09679v1)

[I Know What You Want: Semantic Learning for Text Comprehension](https://arxiv.org/abs/1809.02794v2)

[Gradient-based Inference for Networks with Output Constraints](https://arxiv.org/abs/1707.08608v3)

[Unifying Question Answering and Text Classification via Span Extraction](https://arxiv.org/abs/1904.09286v1)

[Modality Attention for End-to-End Audio-visual Speech Recognition](https://arxiv.org/abs/1811.05250v2)

[The Use of Unlabeled Data versus Labeled Data for Stopping Active Learning for Text Classification](https://arxiv.org/abs/1901.09126v2)

[Efficient and Robust Question Answering from Minimal Context over Documents](https://aclweb.org/anthology/P18-1160)

## Some Interesting or Useful Blogs
[Comparing Pre-trained Language Models with Semantic Parsing](https://jbkjr.com/posts/2019/01/unsupervised_pretraining_comparison/)

[To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/pdf/1903.05987.pdf)

[Jack Koch's Blog](https://jbkjr.com/year-archive/)

[Attention mechanism paper from another repo](https://github.com/yuquanle/Attention-Mechanisms-paper/blob/master/Attention-mechanisms-paper.md)

[NAACL 2019 Accepted Papers](https://naacl2019.org/program/accepted/)

[StateOfTheArt.ai](https://www.stateoftheart.ai/)

[Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)

[INTERSPEECH 2017系列 | 语音识别之后处理技术](https://yq.aliyun.com/articles/332445/)

[Yejin Choi - Language and X ∈ {vision, knowledge, world, mind, society...}](https://homes.cs.washington.edu/~yejin/)

[Better Heatmaps and Correlation Matrix Plots in Python](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec)

[ICLR 2019: Overcoming limited data
Summaries of papers that address learning from few examples](https://towardsdatascience.com/iclr-2019-overcoming-limited-data-382cd19db6d2)

[用Bertviz可视化Position Embedding](https://zhuanlan.zhihu.com/p/58787695)

### [Return Top](https://github.com/SparkJiao/KK-s-Paperlist#kks-paperlist)
