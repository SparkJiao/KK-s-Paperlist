# KK's Paperlist
A list of papers for machine learning, deep learning, reinforcement learning, NLP or something interesting.  
Recommendations or contributions are all welcome!

## Table of Contents
- [Bentchmark or Datasets](https://github.com/SparkJiao/KK-s-Paperlist#benchmark-or-datasets)
- [Language Models](https://github.com/SparkJiao/KK-s-Paperlist#language-models)
- [Visual Question Answering](https://github.com/SparkJiao/KK-s-Paperlist#visual-question-answering)
- [Natural Language Inference](https://github.com/SparkJiao/KK-s-Paperlist#natural-language-inference)
- [Machine Reading Comprehension](https://github.com/SparkJiao/KK-s-Paperlist#machine-reading-comprehension)
- [Commonsense Reasoning](https://github.com/SparkJiao/KK-s-Paperlist#commonsense-reasoning)
- [Machine Translation](https://github.com/SparkJiao/KK-s-Paperlist#machine-translation)
- [Interpretability of Machine Learning](https://github.com/SparkJiao/KK-s-Paperlist#interpretability-of-machine-learning)
- [Multi-Task Learning](https://github.com/SparkJiao/KK-s-Paperlist#multi-task-learning)
- [Attention Mechanism](https://github.com/SparkJiao/KK-s-Paperlist#attention-mechanism)
- [Reinforcement Learning](https://github.com/SparkJiao/KK-s-Paperlist#reinforcement-learning)
- [Dialog System](https://github.com/SparkJiao/KK-s-Paperlist#dialog-system)
- [Graph Neural Network](https://github.com/SparkJiao/KK-s-Paperlist#graph-neural-network)
- [GAN](https://github.com/SparkJiao/KK-s-Paperlist#gan)
- [Theory](https://github.com/SparkJiao/KK-s-Paperlist#theory)
- [Waiting List](https://github.com/SparkJiao/KK-s-Paperlist#waiting-list)
- [Some Interesting or Useful Blogs](https://github.com/SparkJiao/KK-s-Paperlist#some-interesting-or-useful-blogs)

## Benchmark or Datasets
[CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog](https://arxiv.org/abs/1903.03166)

[DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension](https://arxiv.org/abs/1902.00164)

[DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://arxiv.org/abs/1903.00161)

## Language Models

[Conditional BERT Contextual Augmentation](https://arxiv.org/abs/1812.06705)

## Visual Question Answering
[CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog](https://arxiv.org/abs/1903.03166)

[Visual Dialog](https://arxiv.org/abs/1611.08669)
- Late Fusion
- Hierarchical Recurrent Encoder
- Memory Network
- [Github is here.](https://github.com/batra-mlp-lab/visdial)  

[Learning to Reason: End-to-End Module Networks for Visual Question Answering](https://github.com/ronghanghu/n2nmn)  

[Visual Coreference Resolution in Visual Dialog using Neural Module Networks](https://github.com/facebookresearch/corefnmn)

## Natural Language Inference
[DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching](https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangKun.5147.pdf)

[Neural Natural Language Inference Models Enhanced with External Knowledge](https://aclweb.org/anthology/P18-1224)

[Knowledge Base Relation Detection via Multi-View Matching](https://arxiv.org/pdf/1803.00612.pdf)

## Machine Reading Comprehension
[DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension](https://arxiv.org/abs/1902.00164)

[Review Conversational Reading Comprehension](https://arxiv.org/abs/1902.00821)

[FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://arxiv.org/abs/1810.06683)
- Using RNN to grasp historical information in conversational question answering.

[SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering](https://arxiv.org/abs/1812.03593)
- Generate bert embedding for reading comprehensing and question answering.

[FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension](https://arxiv.org/abs/1711.07341)
- Extend existing attention approaches from three perspectives.

[Densely Connected Attention Propagation for Reading Comprehension](https://arxiv.org/abs/1811.04210)
- Propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension.

[S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension](https://arxiv.org/abs/1706.04815)

[QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541)

[U-Net: Machine Reading Comprehension with Unanswerable Questions](https://arxiv.org/abs/1810.06638)

[Reinforced Mnemonic Reader for Machine Reading Comprehension](https://arxiv.org/pdf/1705.02798.pdf)

[Read + Verify: Machine Reading Comprehension with Unanswerable Questions](https://arxiv.org/pdf/1808.05759.pdf)

[Multihop Attention Networks for Question Answer Matching](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/Multihop%20Attention%20Networks%20for%20Question%20Answer%20Matching.pdf)

[DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://arxiv.org/abs/1903.00161)

[Multi-Matching Network for Multiple Choice Reading Comprehension](http://xplan-lab.org/Paper_PDF/AAAI-19.pdf)

[Option Comparison Network for Multiple-choice Reading Comprehension](https://arxiv.org/abs/1903.03033)

[Dual Co-Matching Network for Multi-choice Reading Comprehension](https://arxiv.org/abs/1901.09381)

[Hierarchical Attention Flow for Multiple-Choice Reading Comprehension](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16331/16177)

[Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases](https://arxiv.org/pdf/1903.02188.pdf)

[SQuAD-MARS](https://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/80013584)

[Coarse-to-Fine Question Answering for Long Documents](https://aclweb.org/anthology/P17-1020)

[Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions](https://arxiv.org/pdf/1811.08610.pdf)

## Commonsense Reasoning

[Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)

## Dialog System
[An Efficient Approach to Encoding Context for Spoken Language Understanding](https://arxiv.org/pdf/1807.00267.pdf)

[The Second Conversational Intelligence Challenge (ConvAI2)](https://arxiv.org/pdf/1902.00098.pdf)

[A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues](https://arxiv.org/pdf/1605.06069v3.pdf)

[Memory-augmented Dialogue Management for Task-oriented Dialogue Systems](https://arxiv.org/abs/1805.00150)


## Attention Mechanism
[An Introductory Survey on Attention Mechanisms in NLP Problems](https://arxiv.org/abs/1811.05544)

[Modeling Localness for Self-Attention Networks](https://arxiv.org/abs/1810.10182)

[Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling](https://arxiv.org/abs/1809.01557)

[How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues](https://www.csie.ntu.edu.tw/~yvchen/doc/NAACL18_TimeDecay.pdf)

[Context-Aware Self-Attention Networks](https://arxiv.org/pdf/1902.05766.pdf)
- Use summarized vectors of context or hidden states of context to add extra contextual information into the process of calculating the similarity of between each word in KEY and VALUE.

[Document Modeling with External Attention for Sentence Extraction](http://aclweb.org/anthology/P18-1188)


## Machine Translation
[DTMT: A Novel Deep Transition Architecture for Neural Machine Translation](https://arxiv.org/pdf/1812.07807.pdf)
- Tap the potential strength of deep transition between consecutive hidden states and propose a novel deep transition RNN-based architecture for NMT
- Propose a simple yet more effective linear transformation enhanced GRU for our deep transition RNMT, which provides a linear transformation path for deep transition of consecutive hidden states.

## Interpretability of Machine Learning
[Interpretable machine learning: deÔ¨Ånitions, methods, and applications](https://arxiv.org/pdf/1901.04592.pdf)

## Multi-Task Learning
[Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/pdf/1901.11504.pdf)
- Better performance than BERT on natural language understanding tasks.

[Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering](https://arxiv.org/pdf/1812.02354.pdf)

## Reinforcement Learning
[Learning Structured Representation for Text Classification via Reinforcement Learning](http://coai.cs.tsinghua.edu.cn/hml/media/files/AAAI2018_ClassifyAndStructure.pdf)

[Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data](https://arxiv.org/abs/1903.06164)

## Graph Neural Network
[Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153)

## GAN

[Adversarial Training Methods for Semi-Supervised Text Classification](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/ADVERSARIAL%20TRAINING%20METHODS%20FOR%20SEMI-SUPERVISED%20TEXT%20CLASSIFICATION.pdf)

[Adversarial Examples for Natural Language Classification Problems](https://github.com/SparkJiao/KK-s-Paperlist/blob/master/papers/Adversarial%20Examples%20for%20Natural%20Language%20Classification%20Problems.pdf)

## Theory
[On the Impact of the Activation Function on Deep Neural Networks Training](https://arxiv.org/pdf/1902.06853.pdf)

## Waiting List
[BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671)

[Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing](https://www.semanticscholar.org/paper/Attention%2C-please!-A-Critical-Review-of-Neural-in-Galassi-Lippi/9a7def005efb5b4984886c8a07ec4d80152602ab)

[Fast LSTMs in PyTorch](https://lernapparat.de/fast-lstm-pytorch/)

[Random Language Model](https://arxiv.org/abs/1809.01201v2)

## Some Interesting or Useful Blogs
[Comparing Pre-trained Language Models with Semantic Parsing](https://jbkjr.com/posts/2019/01/unsupervised_pretraining_comparison/)

[To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/pdf/1903.05987.pdf)

[Jack Koch's Blog](https://jbkjr.com/year-archive/)

[Attention mechanism paper from another repo](https://github.com/yuquanle/Attention-Mechanisms-paper/blob/master/Attention-mechanisms-paper.md)

[NAACL 2019 Accepted Papers](https://naacl2019.org/program/accepted/)

[StateOfTheArt.ai](https://www.stateoftheart.ai/)

[Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)

[INTERSPEECH 2017Á≥ªÂàó | ËØ≠Èü≥ËØÜÂà´‰πãÂêéÂ§ÑÁêÜÊäÄÊúØ](https://yq.aliyun.com/articles/332445/)

[Yejin Choi - Language and X ‚àà {vision, knowledge, world, mind, society...}](https://homes.cs.washington.edu/~yejin/)

### [Return Top](https://github.com/SparkJiao/KK-s-Paperlist#table-of-contents)
